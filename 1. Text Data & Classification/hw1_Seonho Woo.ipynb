{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 1\n",
    "### Seonho Woo (Claire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/clairewoo/.venv/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import sys\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Representing Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0    The only thing I got from college is a caffein...\n",
       " 1    I love it when professors draw a big question ...\n",
       " 2    Remember the hundred emails from companies whe...\n",
       " 3    Today my pop-pop told me I was not “forced” to...\n",
       " 4    @VolphanCarol @littlewhitty @mysticalmanatee I...\n",
       " Name: text, dtype: object,\n",
       " 0    1\n",
       " 1    1\n",
       " 2    1\n",
       " 3    1\n",
       " 4    1\n",
       " Name: sarcastic, dtype: int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text = pd.read_csv('si630w23-hw1.train.csv')['text']\n",
    "train_label = pd.read_csv('si630w23-hw1.train.csv')['sarcastic']\n",
    "\n",
    "train_text.head(), train_label.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((280,), (280,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text = pd.read_csv('si630w23-hw1.dev.csv')['text']\n",
    "test_label = pd.read_csv('si630w23-hw1.dev.csv')['sarcastic']\n",
    "\n",
    "np.shape(test_text), np.shape(test_label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.1 - Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(string):\n",
    "    tokens = string.split(\" \")\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example:\n",
    "# str = \"I would like to have some teas, and it would be great if it is typa milk tea, 20.99\"\n",
    "# tokenize(str)\n",
    "\n",
    "token_row = []\n",
    "for i in range(len(train_text)):\n",
    "    token_row.append(tokenize(train_text[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def better_tokenize(str):\n",
    "    str = str.lower()\n",
    "    str = re.sub(r'[^A-Za-z0-9_]', ' ', str)\n",
    "    #str = replace(\"\"\"[\\p{P}\\p{S}&&[^.]]+\"\"\".toRegex(), \"\")\n",
    "    tokens = str.split(\" \")\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "better_token_list = [better_tokenize(word) for word in train_text]\n",
    "all_tokens = list(chain.from_iterable(better_token_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabs = []\n",
    "all_tokens = list(chain.from_iterable(better_token_list))\n",
    "freq = []\n",
    "\n",
    "for word in list(set(all_tokens)):\n",
    "    if all_tokens.count(word) >= 2:\n",
    "        vocabs.append(word)\n",
    "\n",
    "for word in vocabs:\n",
    "    freq.append(better_token_list.count(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csc_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Giving rows and columns values\n",
    "'''\n",
    "rows = np.array([0, 1, 0, 2, 1, 1])\n",
    "columns = np.array([1, 0, 0, 2, 1, 2])\n",
    "\n",
    "# Giving array data\n",
    "arrayData = np.array([1, 3, 2, 5, 7, 6])\n",
    "\n",
    "# Using csc_matrix function to create a 3x3 sparse matrix in column format\n",
    "sparse_matrix = csc_matrix((arrayData, (rows, columns)),\n",
    "   shape = (3, 3)).toarray()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor text in train_text:\\n    for token in list(set(tokens)):\\n        if tokens.count(token) >= 2:\\n            row_idx.append()= np.array()\\n            X[i][j] += 1\\n        row_idx.append(vocabs.index(tok)[0])\\n        col_idx.append(vocabs.index(tok)[1])\\n        data.append(token.count(tok))\\n\\n\\n        # find index of this token from vocab list\\n        # X[i][j] += 1\\n        # Or record row idx/col idx/val\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "for text in train_text:\n",
    "    for token in list(set(tokens)):\n",
    "        if tokens.count(token) >= 2:\n",
    "            row_idx.append()= np.array()\n",
    "            X[i][j] += 1\n",
    "        row_idx.append(vocabs.index(tok)[0])\n",
    "        col_idx.append(vocabs.index(tok)[1])\n",
    "        data.append(token.count(tok))\n",
    "\n",
    "\n",
    "        # find index of this token from vocab list\n",
    "        # X[i][j] += 1\n",
    "        # Or record row idx/col idx/val\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = {\n",
    "    'doc1': ['python', 'text', 'data', 'nlp', 'data', 'matrix', 'mining'],\n",
    "    'doc2': ['data', 'science', 'data', 'processing', 'cleaning', 'data'],\n",
    "    'doc3': ['r', 'data', 'science', 'text', 'mining', 'nlp'],\n",
    "    'doc4': ['programming', 'c', 'algorithms', 'data', 'structures'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([['python', 'text', 'data', 'nlp', 'data', 'matrix', 'mining'], ['data', 'science', 'data', 'processing', 'cleaning', 'data'], ['r', 'data', 'science', 'text', 'mining', 'nlp'], ['programming', 'c', 'algorithms', 'data', 'structures']])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nn_nonzero = 0\\nvocab = set()\\nfor docterms in docs.values():\\n    unique_terms = set(docterms)    # all unique terms of this doc\\n    vocab.union(unique_terms)         # set union: add unique terms of this doc\\n    n_nonzero += len(unique_terms)  # add count of unique terms in this doc\\n\\n# make a list of document names\\n# the order will be the same as in the dict\\ndocnames = list(docs.keys())\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###\n",
    "'''\n",
    "n_nonzero = 0\n",
    "vocab = set()\n",
    "for docterms in docs.values():\n",
    "    unique_terms = set(docterms)    # all unique terms of this doc\n",
    "    vocab.union(unique_terms)         # set union: add unique terms of this doc\n",
    "    n_nonzero += len(unique_terms)  # add count of unique terms in this doc\n",
    "\n",
    "# make a list of document names\n",
    "# the order will be the same as in the dict\n",
    "docnames = list(docs.keys())\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_val = 0\n",
    "vocabs = set()\n",
    "for word in better_token_list:\n",
    "    uniq = set(word)\n",
    "    #print(uniq, type(uniq))\n",
    "    vocabs = vocabs | uniq\n",
    "    #print(vocabs)\n",
    "    n_val += len(uniq)\n",
    "\n",
    "sentence = np.arange(1, len(better_token_list), 1)\n",
    "vocabs = np.array(list(vocabs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabs_sort = np.argsort(vocabs)\n",
    "nsent = len(sentence)\n",
    "nvoc = len(vocabs)\n",
    "\n",
    "data_val = np.empty(n_val, dtype=np.intc)     # all non-zero term frequencies at data[k]\n",
    "row_idx = np.empty(n_val, dtype=np.intc)     # row index for kth data item (kth term freq.)\n",
    "col_idx = np.empty(n_val, dtype=np.intc)     # column index for kth data item (kth term freq.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3466, 10507)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nsent, nvoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nvocab_sorter = np.argsort(vocab)    # indices that sort \"vocab\"\\nndocs = len(docnames)\\nnvocab = len(vocab)\\n\\ndata = np.empty(n_nonzero, dtype=np.intc)     # all non-zero term frequencies at data[k]\\nrows = np.empty(n_nonzero, dtype=np.intc)     # row index for kth data item (kth term freq.)\\ncols = np.empty(n_nonzero, dtype=np.intc)     # column index for kth data item (kth term freq.)\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###\n",
    "###\n",
    "#docnames = np.array(docnames)\n",
    "#vocab = np.array(list(vocab)) \n",
    "'''\n",
    "vocab_sorter = np.argsort(vocab)    # indices that sort \"vocab\"\n",
    "ndocs = len(docnames)\n",
    "nvocab = len(vocab)\n",
    "\n",
    "data = np.empty(n_nonzero, dtype=np.intc)     # all non-zero term frequencies at data[k]\n",
    "rows = np.empty(n_nonzero, dtype=np.intc)     # row index for kth data item (kth term freq.)\n",
    "cols = np.empty(n_nonzero, dtype=np.intc)     # column index for kth data item (kth term freq.)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0     # current index in the sparse matrix data\n",
    "# go through all documents with their words\n",
    "for sent in sentence:\n",
    "    for word in vocabs:\n",
    "        # find indices into  such that, if the corresponding elements in  were\n",
    "        # inserted before the indices, the order of  would be preserved\n",
    "        word_indices = vocabs_sort[np.searchsorted(vocabs, word, sorter=vocabs_sort)]\n",
    "\n",
    "        # count the unique words of the document and get their vocabulary indices\n",
    "        uniq_indices, counts = np.unique(word_indices, return_counts=True)\n",
    "        #n_vals = len(uniq_indices)  # = number of unique words\n",
    "        idx_end = idx + len(uniq_indices)  #  to  is the slice that we will fill with data\n",
    "\n",
    "        data_val[idx:idx_end] = counts                  # save the counts (word frequencies)\n",
    "        col_idx[idx:idx_end] = uniq_indices            # save the column index: index in \n",
    "        doc_idx = np.where(sentence == sent)     # get the document index for the document name\n",
    "        row_idx[idx:idx_end] = np.repeat(doc_idx, len(uniq_indices))  # save it as repeated value\n",
    "\n",
    "        idx = idx_end  # resume with next document -> add data to the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 7)\t1\n",
      "  (0, 8)\t1\n",
      "  (0, 9)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 11)\t1\n",
      "  (0, 12)\t1\n",
      "  (0, 13)\t1\n",
      "  (0, 14)\t1\n",
      "  (0, 15)\t1\n",
      "  (0, 16)\t1\n",
      "  (0, 17)\t1\n",
      "  (0, 18)\t1\n",
      "  (0, 19)\t1\n",
      "  (0, 20)\t1\n",
      "  (0, 21)\t1\n",
      "  (0, 22)\t1\n",
      "  (0, 23)\t1\n",
      "  (0, 24)\t1\n",
      "  :\t:\n",
      "  (6, 1129)\t1\n",
      "  (6, 1130)\t1\n",
      "  (6, 1131)\t1\n",
      "  (6, 1132)\t1\n",
      "  (6, 1133)\t1\n",
      "  (6, 1134)\t1\n",
      "  (6, 1135)\t1\n",
      "  (6, 1136)\t1\n",
      "  (6, 1137)\t1\n",
      "  (6, 1138)\t1\n",
      "  (6, 1139)\t1\n",
      "  (6, 1140)\t1\n",
      "  (6, 1141)\t1\n",
      "  (6, 1142)\t1\n",
      "  (6, 1143)\t1\n",
      "  (6, 1144)\t1\n",
      "  (6, 1145)\t1\n",
      "  (6, 1146)\t1\n",
      "  (6, 1147)\t1\n",
      "  (6, 1148)\t1\n",
      "  (6, 1149)\t1\n",
      "  (6, 1150)\t1\n",
      "  (6, 1151)\t1\n",
      "  (6, 1152)\t1\n",
      "  (6, 1153)\t1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3466, 10507)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm = sparse.coo_matrix((data_val, (row_idx, col_idx)), shape=(nsent, nvoc), dtype=np.intc)\n",
    "print(dtm)\n",
    "np.shape(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# row indices\n",
    "#row_idx = np.array(vocabs)\n",
    "# column indices\n",
    "#col_idx = np.arange(0, len(train_text), dtype=int)\n",
    "# data to be stored in COO sparse matrix\n",
    "#data = np.zeros((len(vocabs), len(train_text)))\n",
    "#np.shape(row_idx), np.shape(col_idx), np.shape(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n## Dense Matrix of X\\nfor i in range(np.shape(data)[0]): # 4009\\n    for j in range(np.shape(data)[1]): # 3467\\n        count = better_token_list[j].count(eff_uniq_tokens[i])\\n        data[i][j] = int(count)\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "## Dense Matrix of X\n",
    "for i in range(np.shape(data)[0]): # 4009\n",
    "    for j in range(np.shape(data)[1]): # 3467\n",
    "        count = better_token_list[j].count(eff_uniq_tokens[i])\n",
    "        data[i][j] = int(count)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Logistic Regression in *numpy*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining functions of Sigmoid, Log-likelihood, Gradient, Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    result = 1 / (1+np.exp(-x))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(x, y, beta):\n",
    "    gradient = np.matmul((sigmoid(beta.dot(x))-y), y)\n",
    "\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(X, y, beta):\n",
    "    ll = sum([y[i]*np.log(sigmoid(beta.dot(X[i]))) + (1-y[i])*np.log(1-sigmoid(beta.dot(X[i]))) for i in range(len(y))])\n",
    "    \n",
    "    return ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(X: np.ndarray, y: np.ndarray, learning_rate: float, num_step: int) -> float:\n",
    "\n",
    "    np.random.seed(630)\n",
    "\n",
    "    converg = False\n",
    "    n = X.shape[0] # number of samples\n",
    "    #loss_list = []\n",
    "    beta = np.zeros(X.shape[1]) #np.random.random(X.shape[1])\n",
    "    temp = beta\n",
    "    ll_pre = -1e10\n",
    "    \n",
    "    while not converg:\n",
    "        for epoch in range(num_step):\n",
    "            for i in range(n):     \n",
    "                # Update temp for beta\n",
    "                temp = beta - learning_rate*compute_gradient(X[i], y[i], beta)\n",
    "                # Update beta from temp\n",
    "                beta = temp \n",
    "            ll = log_likelihood(X, y, beta)\n",
    "\n",
    "            if (abs(ll - ll_pre) <= 1e-5):\n",
    "                converge = True\n",
    "                break\n",
    "            ll_pre = ll\n",
    "\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "better_token_list = [better_tokenize(word) for word in train_text]\n",
    "all_tokens = list(chain.from_iterable(better_token_list))\n",
    "uniq_tokens = list(set(all_tokens))\n",
    "\n",
    "eff_uniq_tokens = []\n",
    "\n",
    "for word in uniq_tokens:\n",
    "    if all_tokens.count(word) >= 2:\n",
    "        #print(word)\n",
    "        eff_uniq_tokens.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text, beta):\n",
    "    tokens = better_tokenize(text)\n",
    "    uniq = set(tokens)\n",
    "    x = np.zeros(len(uniq))\n",
    "\n",
    "    data_val = np.empty(n_val, dtype=np.intc)     # all non-zero term frequencies at data[k]\n",
    "    row_idx = np.empty(n_val, dtype=np.intc)     # row index for kth data item (kth term freq.)\n",
    "    col_idx = np.empty(n_val, dtype=np.intc)  \n",
    "\n",
    "    for token, i in enumerate(tokens):\n",
    "        # count the unique words of the document and get their vocabulary indices\n",
    "        uniq_indices, counts = np.unique(word_indices, return_counts=True)\n",
    "        #n_vals = len(uniq_indices)  # = number of unique words\n",
    "        idx_end = idx + len(uniq_indices)  #  to  is the slice that we will fill with data\n",
    "\n",
    "        data_val[idx:idx_end] = counts                  # save the counts (word frequencies)\n",
    "        col_idx[idx:idx_end] = uniq_indices            # save the column index: index in \n",
    "        doc_idx = np.where(sentence == sent)     # get the document index for the document name\n",
    "        row_idx[idx:idx_end] = np.repeat(doc_idx, len(uniq_indices))  # save it as repeated value\n",
    "\n",
    "        row_val = np.where(sentence == sent) # find the index of this token from vocab\n",
    "        col_val = np.where()\n",
    "        x[i] += 1\n",
    "    \n",
    "    return int(sigmoid(beta.dot(x)) >= 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Logistic Regression with *PyTorch*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_sparse_tensor(dtm: np.ndarray):\n",
    "    return torch.from_numpy(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        outputs = torch.sigmoid(self.linear(x))\n",
    "        return outputs\n",
    "\n",
    "\n",
    "        return self.final_layer(layer_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_dim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m LogisticRegression(input_dim,output_dim)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'input_dim' is not defined"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(input_dim,output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m      2\u001b[0m model\u001b[39m.\u001b[39meval()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39m#batch_size=32\u001b[39;00m\n\u001b[1;32m      7\u001b[0m criterion \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mBCELoss()\n\u001b[0;32m----> 8\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mSGD(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39mlearning_rate, momentum\u001b[39m=\u001b[39m\u001b[39m0.9\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "epochs = 1000\n",
    "learning_rate = torch.tensor(1/1e3) # 0.001\n",
    "#batch_size=32\n",
    "\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many dimensions 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X_train, X_test \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mTensor(train_text),torch\u001b[39m.\u001b[39mTensor(test_text)\n\u001b[1;32m      2\u001b[0m y_train, y_test \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mTensor(train_label), torch\u001b[39m.\u001b[39mTensor(test_text)\n",
      "\u001b[0;31mValueError\u001b[0m: too many dimensions 'str'"
     ]
    }
   ],
   "source": [
    "X_train, X_test = torch.Tensor(train_text),torch.Tensor(test_text)\n",
    "y_train, y_test = torch.Tensor(train_label), torch.Tensor(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tqdm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m iterations \u001b[39m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m \u001b[39miter\u001b[39m \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m----> 5\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(\u001b[39mint\u001b[39m(epochs)),desc\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTraining Epochs\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m      6\u001b[0m     x \u001b[39m=\u001b[39m X_train\n\u001b[1;32m      7\u001b[0m     labels \u001b[39m=\u001b[39m y_train\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tqdm' is not defined"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "losses_test = []\n",
    "iterations = []\n",
    "iter = 0\n",
    "for epoch in tqdm(range(int(epochs)),desc='Training Epochs'):\n",
    "    x = X_train\n",
    "    labels = y_train\n",
    "    optimizer.zero_grad() # Setting our stored gradients equal to zero\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(torch.squeeze(outputs), labels) \n",
    "    \n",
    "    loss.backward() # Computes the gradient of the given tensor w.r.t. the weights/bias\n",
    "    \n",
    "    optimizer.step() # Updates weights and biases with the optimizer (SGD)\n",
    "    \n",
    "    iter+=1\n",
    "    if iter%10000==0:\n",
    "        with torch.no_grad():\n",
    "            # Calculating the loss and accuracy for the test dataset\n",
    "            correct_test = 0\n",
    "            total_test = 0\n",
    "            outputs_test = torch.squeeze(model(X_test))\n",
    "            loss_test = criterion(outputs_test, y_test)\n",
    "            \n",
    "            predicted_test = outputs_test.round().detach().numpy()\n",
    "            total_test += y_test.size(0)\n",
    "            correct_test += np.sum(predicted_test == y_test.detach().numpy())\n",
    "            accuracy_test = 100 * correct_test/total_test\n",
    "            losses_test.append(loss_test.item())\n",
    "            \n",
    "            # Calculating the loss and accuracy for the train dataset\n",
    "            total = 0\n",
    "            correct = 0\n",
    "            total += y_train.size(0)\n",
    "            correct += np.sum(torch.squeeze(outputs).round().detach().numpy() == y_train.detach().numpy())\n",
    "            accuracy = 100 * correct/total\n",
    "            losses.append(loss.item())\n",
    "            iterations.append(iter)\n",
    "            \n",
    "            print(f\"Iteration: {iter}. \\nTest - Loss: {loss_test.item()}. Accuracy: {accuracy_test}\")\n",
    "            print(f\"Train -  Loss: {loss.item()}. Accuracy: {accuracy}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 21\u001b[0m\n\u001b[1;32m     17\u001b[0m     plt\u001b[39m.\u001b[39mshow()\n\u001b[1;32m     20\u001b[0m \u001b[39m# Train Data\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m model_plot(model,X_train,y_train,\u001b[39m'\u001b[39m\u001b[39mTrain Data\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[39m# Test Dataset Results\u001b[39;00m\n\u001b[1;32m     24\u001b[0m model_plot(model,X_test,y_test,\u001b[39m'\u001b[39m\u001b[39mTest Data\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "def model_plot(model,X,y,title):\n",
    "    parm = {}\n",
    "    b = []\n",
    "    for name, param in model.named_parameters():\n",
    "        parm[name]=param.detach().numpy()  \n",
    "    \n",
    "    w = parm['linear.weight'][0]\n",
    "    b = parm['linear.bias'][0]\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y,cmap='jet')\n",
    "    u = np.linspace(X[:, 0].min(), X[:, 0].max(), 2)\n",
    "    plt.plot(u, (0.5-b-w[0]*u)/w[1])\n",
    "    plt.xlim(X[:, 0].min()-0.5, X[:, 0].max()+0.5)\n",
    "    plt.ylim(X[:, 1].min()-0.5, X[:, 1].max()+0.5)\n",
    "    plt.xlabel(r'$\\boldsymbol{x_1}$',fontsize=16) # Normally you can just add the argument fontweight='bold' but it does not work with latex\n",
    "    plt.ylabel(r'$\\boldsymbol{x_2}$',fontsize=16)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Train Data\n",
    "model_plot(model,X_train,y_train,'Train Data')\n",
    "\n",
    "# Test Dataset Results\n",
    "model_plot(model,X_test,y_test,'Test Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3d17d5e1495c004c8506537a1ab2a320924549ce176751f154faca4083ddde23"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
